{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d44e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import io\n",
    "import imageio\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import*\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf61161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "n_samples = 25\n",
    "#look at the samples of mnist\n",
    "for i in range(n_samples):\n",
    "    plt.subplot(5, 5, 1 + i)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x_train[i], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a983bf",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7475de86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define discriminator model\n",
    "def define_discriminator():\n",
    "    discriminator = Sequential([\n",
    "    Conv2D(128, (3,3), strides=(2, 2), padding='same', input_shape=(28,28,1)),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Conv2D(128, (3,3), strides=(2, 2), padding='same'),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Dropout(0.4),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(64),\n",
    "    Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "    return discriminator\n",
    "\n",
    "#build discriminztor\n",
    "d_model = define_discriminator()\n",
    "\n",
    "#look at architecture of discriminator\n",
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d372f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define generator model\n",
    "def define_generator(latent_dim):\n",
    "    generator = Sequential([\n",
    "    Dense(128 * 7, input_dim=latent_dim),\n",
    "    Dense(128 * 7 * 7),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    Reshape((7, 7, 128)),\n",
    "    \n",
    "    Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "    \n",
    "    Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'),\n",
    "    LeakyReLU(alpha=0.2),\n",
    "\n",
    "    Conv2D(1, (7,7), activation='sigmoid', padding='same')\n",
    "    ])\n",
    "    \n",
    "    return generator\n",
    "\n",
    "#set the constant\n",
    "latent_dim = 100\n",
    "#build generator\n",
    "g_model = define_generator(latent_dim)\n",
    "#look at architecture of generator\n",
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fbf9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define GAN model\n",
    "def define_gan(g_model, d_model):\n",
    "    d_model.trainable = False\n",
    "\n",
    "    gan = Sequential()\n",
    "    gan.add(g_model)\n",
    "    gan.add(d_model)\n",
    "\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "    return gan\n",
    "\n",
    "#create GAN\n",
    "gan_model = define_gan(g_model, d_model)\n",
    "#look at GAN architecture\n",
    "gan_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8719b3",
   "metadata": {},
   "source": [
    "# Helping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08258f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which load and preprocess train samples\n",
    "def load_real_samples():\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = np.expand_dims(x_train / 255, axis=-1).astype('float32')\n",
    "    return x_train\n",
    "\n",
    "#function which select random samples from train samples\n",
    "def select_real_samples(dataset, quantity):\n",
    "    selected_indexes = [random.randint(0, len(dataset) - 1) for _ in range(quantity)]\n",
    "    selected_samples = dataset[selected_indexes]\n",
    "    label_real = np.ones((quantity, 1))\n",
    "    return selected_samples, label_real\n",
    "\n",
    "#function which generate points in latent space\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    points = np.random.randn(latent_dim * n_samples)\n",
    "    points = points.reshape(n_samples, latent_dim)\n",
    "    return points\n",
    "\n",
    "#function which generate fake samples\n",
    "def generate_fake_samples(g_model, latent_dim, n_samples):\n",
    "    points = generate_latent_points(latent_dim, n_samples)\n",
    "    samples = g_model.predict(points)\n",
    "    real_labels = np.zeros((n_samples, 1))\n",
    "    return samples, real_labels\n",
    "\n",
    "#function to display generated images\n",
    "def show_images(examples, n):\n",
    "    for i in range(n * n):\n",
    "        plt.subplot(n, n, 1 + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
    "    plt.show()\n",
    "\n",
    "#fuction to save 'subplot' images\n",
    "def save_images(examples, n):\n",
    "    a = io.BytesIO()\n",
    "    plt.figure()\n",
    "    for i in range(n * n):\n",
    "        plt.subplot(n, n, 1 + i)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(examples[i, :, :, 0], cmap='gray_r')\n",
    "    plt.savefig(a, format='png')\n",
    "    plt.close()\n",
    "    a.seek(0)\n",
    "    image = plt.imread(a)\n",
    "    combined_image = np.asarray(image)\n",
    "    return combined_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d1ab7",
   "metadata": {},
   "source": [
    "# Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_disc_per_epoch = []\n",
    "loss_gan_per_epoch = []\n",
    "loss_disc_per_batch = []\n",
    "loss_gan_per_batch = []\n",
    "predictions = []\n",
    "#function to train GAN\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, epochs=150, batches=256):\n",
    "    number_batches = int(dataset.shape[0] / batches)\n",
    "    batches //= 2\n",
    "    global loss_disc_per_epoch, loss_gan_per_epoch, loss_disc_per_batch, loss_gan_per_batch, predictions\n",
    "    latent_points = generate_latent_points(100, 25)\n",
    "    #training by epochs\n",
    "    for i in range(epochs):\n",
    "        #trainig by batches in epoch\n",
    "        for j in range(number_batches):\n",
    "            #traning\n",
    "            X_real, y_real = select_real_samples(dataset, batches)\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, batches)\n",
    "            X, y = np.vstack((X_real, X_fake)), np.vstack((y_real, y_fake))\n",
    "            d_loss, _ = d_model.train_on_batch(X, y)\n",
    "            X_gan = generate_latent_points(latent_dim, batches)\n",
    "            y_gan = np.ones((batches, 1))\n",
    "            gan_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "            #print\n",
    "            print(f'>{i+1}, {j+1}/{number_batches}, \\n         disc loss={d_loss}, \\n         gan loss={gan_loss}')\n",
    "            #loss\n",
    "            loss_disc_per_batch.append(d_loss)\n",
    "            loss_gan_per_batch.append(gan_loss)\n",
    "\n",
    "        #loss\n",
    "        loss_disc_per_epoch.append(d_loss)\n",
    "        loss_gan_per_epoch.append(gan_loss)\n",
    "        #show epoch result\n",
    "        X = g_model.predict(latent_points)\n",
    "        show_images(X, 5)\n",
    "        #save epoch result\n",
    "        predicted_image = save_images(X, 5)\n",
    "        predictions.append(predicted_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75959754",
   "metadata": {},
   "source": [
    "# Train models \\ Saving weights \\ Loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "dataset = load_real_samples()\n",
    "#train GAN\n",
    "train(g_model, d_model, gan_model, dataset, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace0bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathes\n",
    "base_path = ''\n",
    "gan_weights = base_path + 'gan_model.h5'\n",
    "g_weights = base_path + 'g_model.h5'\n",
    "d_weights = base_path + 'd_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e59a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model weights\n",
    "gan_model.save_weights(gan_weights)\n",
    "g_model.save_weights(g_weights)\n",
    "d_model.save_weights(d_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f76dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading model weights\n",
    "gan_model.load_weights(gan_weights)\n",
    "g_model.load_weights(g_weights)\n",
    "d_model.load_weights(d_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4fa88b",
   "metadata": {},
   "source": [
    "# Look at results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff0b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_points = generate_latent_points(100, 36)\n",
    "X = g_model.predict(latent_points)\n",
    "show_images(X, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977bff65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#losses per batches\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter([i for i in range(len(loss_disc_per_batch))], loss_disc_per_batch)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter([i for i in range(len(loss_gan_per_batch))], loss_gan_per_batch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e32c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#losses per epochs\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter([i for i in range(len(loss_disc_per_epoch))], loss_disc_per_epoch)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter([i for i in range(len(loss_gan_per_epoch))], loss_gan_per_epoch)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc303791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a gif\n",
    "output_file = base_path + 'animation.gif'\n",
    "imageio.mimsave(output_file, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dbce6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d323d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b3fa5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
